________________________________________________
ls: Lists files and directories in the current directory. 
________________________________________________
pwd: Prints the current working directory. 
________________________________________________
cat > /home/cloudera/processFile1.txt	:Creates a new file named processFile1.txt 
________________________________________________
MITAOE
MIT AOE Alandi Pune
MIT AOE Alandi Pune
Pune Maharashtra


________________________________________________
cat /home/cloudera/processFile1.txt	:View the Contents of the File: 
________________________________________________
hdfs dfs -ls	:List Files in the HDFS Home Directory: 
________________________________________________
hdfs dfs -ls 	/:List Files in the Root Directory of HDFS: 
________________________________________________
hdfs dfs -mkdir /user/cloudera/inputFolder1	:Create a New Directory in Your HDFS Home Directory 
________________________________________________
hdfs dfs -put /home/cloudera/processFile1.txt /user/cloudera/inputFolder1/	:Upload the File to 
________________________________________________
HDFS (inputFolder1 Directory): 
hdfs dfs -cat /user/cloudera/inputFolder1/processFile1.txt	:Display the Contents of the Uploaded 
________________________________________________
File in HDFS: 
hadoop jar /home/cloudera/WordCount.jar WordCount /user/cloudera/inputFolder1/processFile1.txt /user/cloudera/out1	:Run the Hadoop WordCount 

________________________________________________
Job: 

hdfs dfs -ls /user/cloudera/out1	:List the Files in the Output Directory 
________________________________________________
hdfs dfs -cat /user/cloudera/out1/part-r-00000	:Display the Results of the WordCount Job:




code:
import java.io.IOException; 
import java.util.StringTokenizer; 
 
import org.apache.hadoop.conf.Configuration; 
import org.apache.hadoop.fs.Path; 
import org.apache.hadoop.io.IntWritable; 
import org.apache.hadoop.io.Text; 
import org.apache.hadoop.mapreduce.Job; 
import org.apache.hadoop.mapreduce.Mapper; 
import org.apache.hadoop.mapreduce.Reducer; 
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat; 
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat; 
 
public class WordCount { 
 
  public static class TokenizerMapper 
       extends Mapper<Object, Text, Text, IntWritable>{ 
 
    private final static IntWritable one = new IntWritable(1); 
    private Text word = new Text(); 
 
    public void map(Object key, Text value, Context context 
                    ) throws IOException, InterruptedException { 
      StringTokenizer itr = new StringTokenizer(value.toString()); 
      while (itr.hasMoreTokens()) { 
        word.set(itr.nextToken()); 
        context.write(word, one); 
      } 
    } 
  } 
 
  public static class IntSumReducer 
       extends Reducer<Text,IntWritable,Text,IntWritable> { 
    private IntWritable result = new IntWritable(); 
 
    public void reduce(Text key, Iterable<IntWritable> values, 
                       Context context 
                       ) throws IOException, InterruptedException { 
      int sum = 0; 
      for (IntWritable val : values) { 
        sum += val.get(); 
      } 
      result.set(sum); 
      context.write(key, result); 
    } 
  } 
 
  public static void main(String[] args) throws Exception { 
    Configuration conf = new Configuration(); 
    Job job = Job.getInstance(conf, "word count"); 
    job.setJarByClass(WordCount.class); 
    job.setMapperClass(TokenizerMapper.class); 
    job.setCombinerClass(IntSumReducer.class); 
    job.setReducerClass(IntSumReducer.class); 
    job.setOutputKeyClass(Text.class); 
    job.setOutputValueClass(IntWritable.class); 
    FileInputFormat.addInputPath(job, new Path(args[0])); 
    FileOutputFormat.setOutputPath(job, new Path(args[1])); 
    System.exit(job.waitForCompletion(true) ? 0 : 1); 
  }