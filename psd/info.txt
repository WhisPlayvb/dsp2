________________________________________________
ls: Lists files and directories in the current directory. 
________________________________________________
pwd: Prints the current working directory. 
________________________________________________
cat > /home/cloudera/processFile1.txt	:Creates a new file named processFile1.txt 
________________________________________________
MITAOE
MIT AOE Alandi Pune
MIT AOE Alandi Pune
Pune Maharashtra


________________________________________________
cat /home/cloudera/processFile1.txt	:View the Contents of the File: 
________________________________________________
hdfs dfs -ls	:List Files in the HDFS Home Directory: 
________________________________________________
hdfs dfs -ls 	/:List Files in the Root Directory of HDFS: 
________________________________________________
hdfs dfs -mkdir /user/cloudera/inputFolder1	:Create a New Directory in Your HDFS Home Directory 
________________________________________________
hdfs dfs -put /home/cloudera/processFile1.txt /user/cloudera/inputFolder1/	:Upload the File to 
________________________________________________
HDFS (inputFolder1 Directory): 
hdfs dfs -cat /user/cloudera/inputFolder1/processFile1.txt	:Display the Contents of the Uploaded 
________________________________________________
File in HDFS: 
hadoop jar /home/cloudera/WordCount.jar WordCount /user/cloudera/inputFolder1/processFile1.txt /user/cloudera/out1	:Run the Hadoop WordCount 

________________________________________________
Job: 

hdfs dfs -ls /user/cloudera/out1	:List the Files in the Output Directory 
________________________________________________
hdfs dfs -cat /user/cloudera/out1/part-r-00000	:Display the Results of the WordCount Job:




code:
import java.io.IOException; 
import java.util.StringTokenizer; 
 
import org.apache.hadoop.conf.Configuration; 
import org.apache.hadoop.fs.Path; 
import org.apache.hadoop.io.IntWritable; 
import org.apache.hadoop.io.Text; 
import org.apache.hadoop.mapreduce.Job; 
import org.apache.hadoop.mapreduce.Mapper; 
import org.apache.hadoop.mapreduce.Reducer; 
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat; 
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat; 
 
public class WordCount { 
 
  public static class TokenizerMapper 
       extends Mapper<Object, Text, Text, IntWritable>{ 
 
    private final static IntWritable one = new IntWritable(1); 
    private Text word = new Text(); 
 
    public void map(Object key, Text value, Context context 
                    ) throws IOException, InterruptedException { 
      StringTokenizer itr = new StringTokenizer(value.toString()); 
      while (itr.hasMoreTokens()) { 
        word.set(itr.nextToken()); 
        context.write(word, one); 
      } 
    } 
  } 
 
  public static class IntSumReducer 
       extends Reducer<Text,IntWritable,Text,IntWritable> { 
    private IntWritable result = new IntWritable(); 
 
    public void reduce(Text key, Iterable<IntWritable> values, 
                       Context context 
                       ) throws IOException, InterruptedException { 
      int sum = 0; 
      for (IntWritable val : values) { 
        sum += val.get(); 
      } 
      result.set(sum); 
      context.write(key, result); 
    } 
  } 
 
  public static void main(String[] args) throws Exception { 
    Configuration conf = new Configuration(); 
    Job job = Job.getInstance(conf, "word count"); 
    job.setJarByClass(WordCount.class); 
    job.setMapperClass(TokenizerMapper.class); 
    job.setCombinerClass(IntSumReducer.class); 
    job.setReducerClass(IntSumReducer.class); 
    job.setOutputKeyClass(Text.class); 
    job.setOutputValueClass(IntWritable.class); 
    FileInputFormat.addInputPath(job, new Path(args[0])); 
    FileOutputFormat.setOutputPath(job, new Path(args[1])); 
    System.exit(job.waitForCompletion(true) ? 0 : 1); 
  }




___________________________________
Code Breakdown
Imports:

The code imports classes from the Hadoop library, such as Mapper, Reducer, and Job, which are used for implementing the MapReduce job.
The StringTokenizer class is used to split input text into words.
TokenizerMapper (Mapper Class):

The TokenizerMapper class extends Mapper<Object, Text, Text, IntWritable>, where:
Input type (Object, Text): The key is the line offset or position in the file (not used here), and the value is the line of text.
Output type (Text, IntWritable): The output key is the word (of type Text), and the output value is the count (of type IntWritable).
Method map:
Reads a line of input (value), tokenizes it into words using a StringTokenizer.
For each word, it writes the word (key) and the count of 1 (value) to the context (which will be passed to the reducer).
context.write(word, one); writes the word with a count of 1.
IntSumReducer (Reducer Class):

The IntSumReducer class extends Reducer<Text, IntWritable, Text, IntWritable>, where:
Input type (Text, IntWritable): The input key is a word (Text), and the input value is an iterable list of word counts (IntWritable).
Output type (Text, IntWritable): The output key is the word (Text), and the output value is the sum of the counts (IntWritable).
Method reduce:
Iterates over the list of word counts for each word (the values parameter).
Sums up the counts.
Writes the word and the total count (result.set(sum)) to the context.
context.write(key, result); outputs the final word count.
Main Method (Job Configuration and Execution):

Configuration (conf): A Configuration object is created to configure the job.
Job Creation (job): The Job instance is configured:
The name of the job is set as "word count".
The TokenizerMapper is set as the mapper class.
The IntSumReducer is set as both the combiner and the reducer class.
The output types are set (Text as key and IntWritable as value).
Input and Output Paths:
FileInputFormat.addInputPath(job, new Path(args[0])); specifies the input path (directory or file) for the job.
FileOutputFormat.setOutputPath(job, new Path(args[1])); specifies the output path.
Job Execution: The job is run using job.waitForCompletion(true). The exit code 0 is returned if the job completes successfully.
Execution Flow:
Mapper Phase:

The input text is read line by line, and each word is tokenized. Each word is assigned a count of 1 and sent to the reducer.
Shuffle and Sort Phase:

Hadoop automatically shuffles and sorts the output from the mapper. The words are grouped together so that all occurrences of a particular word are sent to the same reducer.
Reducer Phase:

The reducer aggregates the counts for each word and computes the total count.
It writes the word and its total count to the output.
Example:
Assume the input file contains the following lines:

Copy code
hello world
hello hadoop
Mapper Output:

(hello, 1)
(world, 1)
(hello, 1)
(hadoop, 1)
After Shuffle and Sort:

Grouped by word:
hello: [1, 1]
world: [1]
hadoop: [1]
Reducer Output:

hello: 2
world: 1
hadoop: 1
Final Output:
Copy code
hello 2
world 1
hadoop 1
Running the Job:
You would run this job on a Hadoop cluster by supplying input and output paths as command-line arguments:
lua
Copy code
hadoop jar wordcount.jar WordCount /input/path /output/path
